{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a311cde-75a3-42c3-9a44-22d069591acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#EDA Summary\n",
    "\t•\tI had the U.S. and French election tables.\n",
    "\t•\tI created flag variables to identify which articles came from which table and then merged them.\n",
    "\t•\tThe combined DataFrame included columns like: URL, date, time, language, description, relevance, and the flag variables.\n",
    "\t•\tI looked at the language column first and saw that the majority of the articles were in English, with around 8% in French.\n",
    "\t•\tI decided to filter and only keep English-language texts, which left me with 3,819 articles.\n",
    "\t•\tI checked for duplicates and found two, so I dropped them and ended up with 3,817 articles.\n",
    "\t•\tThere were two types of relevance columns — one for relevance_description and one for relevance_text.\n",
    "\t•\tI checked for null values and saw that relevance_description had around 2,000 nulls, while relevance_text only had 7 nulls.\n",
    "\t•\tI decided to use the relevance_text column as the truth label for my classifier model, and I’ll generate the missing 7 with an LLM.\n",
    "\t•\tAround 16.9% of the articles were marked as relevant, which is a small proportion — this means I’ll need to use some resampling techniques later.\n",
    "\t•\tThen I looked at the URL column and extracted the publisher from it.\n",
    "\t•\tTurns out we have 15 unique publishers, and the top three overall were Politico, Al Jazeera, and Euronews.\n",
    "\t•\tI also checked which publishers published the most relevant articles — top three were NBC News, Washington Post, and NPR.\n",
    "\t•\tI examined the text length of the articles. It followed a log distribution. Most of the articles had fewer than 2,000 words.\n",
    "\t•\tI then looked at time trends to see when the peaks happened.\n",
    "\t•\tThere was a big spike around November 2024, which aligned with the U.S. elections.\n",
    "\t•\tThere was also a smaller spike around July 2024, which matched the French legislative elections.\n",
    "\t•\tI checked the daily spikes, and they matched up with the actual election days, which is reassuring.\n",
    "\t•\tAround 15% of the articles were flagged as relevant for the U.S. election, while less than 1% were flagged as relevant for the French election.\n",
    "\t•\tThat means it’s going to be harder to detect French election articles due to the smaller number of examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1befe73-0ef9-45ff-9d9d-4f464a6ef3f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "us_df = spark.table(\"int_aa_dev_common_worskapce_catalog.geopol.us_election_text\").toPandas()\n",
    "french_df = spark.table(\"int_aa_dev_common_worskapce_catalog.geopol.french_election_text\").toPandas()\n",
    "\n",
    "# Add one-hot style flags\n",
    "us_df[\"is_us_election\"] = 1\n",
    "us_df[\"is_french_election\"] = 0\n",
    "us_df[\"is_german_election\"] = 0\n",
    "\n",
    "french_df[\"is_us_election\"] = 0\n",
    "french_df[\"is_french_election\"] = 1\n",
    "french_df[\"is_german_election\"] = 0\n",
    "\n",
    "# Combine them\n",
    "df = pd.concat([us_df, french_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39185319-65bb-4ffa-ac4c-226157b4a524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "french_df.relevance_description.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d451d9-9230-4cad-8053-fd2b2cd266b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "french_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b021e4-2d76-495c-927b-c4903e1a6590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b45dda3-fbc2-4334-ad43-359e4e683dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef16631-20f1-4eb9-974c-56752ed21b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817aec40-8a7f-4f7a-ae9c-69d06ab3ce3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b92629a-4b52-4f64-af9a-f5755bac48b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter for eng lang \n",
    "df = df[df[\"language\"] == \"en\"].copy()\n",
    "\n",
    "count = df.count()\n",
    "print(f\"Total count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39b90d4-ee58-4834-b1db-f1c81ef4b6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f8b09a-9f72-42fa-b95b-17d501f6e1da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aaea5b6-f9cc-4234-b691-648aa0484319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d015a2ca-8060-4d92-96fb-2d50e9a0b034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f869512-5783-43f9-af6d-5ebf7b3e308f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"url\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6b5369-78a8-4525-be9a-69b3f28ed538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1afebe-e90a-458e-a77d-f788fc313230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "relevance description vs relevance text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9cc203-9985-4d2c-a5cf-1fbdadea7f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.relevance_description.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18fcbba-d6a4-4ac1-aaa7-6f52a596e6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.relevance_description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710e4b32-1d21-4fba-9e8e-5ba393391b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.relevance_text.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4001a5c6-60f2-49c8-8336-4a7cf95c61b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_truth = df[df.relevance_text.isnull()]\n",
    "generate_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ea7f7b-c543-4485-8ed1-fd097bb1e291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d506353b-3db9-4983-9c3f-113aa84bc88b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# Example: assuming your DataFrame is called df and the URL column is named 'url'\n",
    "def extract_domain(url):\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "# Create a new column with just the domain names\n",
    "df['publisher'] = df['url'].apply(extract_domain)\n",
    "\n",
    "# Count how many unique news sources there are\n",
    "unique_publishers = df['publisher'].nunique()\n",
    "print(f\"Number of unique publishers: {unique_publishers}\")\n",
    "\n",
    "# Optional: See the most common publishers\n",
    "print(df['publisher'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a2c7beb-78fd-4b41-a294-45832c81106d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "relevance by publisher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9d08e2-d89f-4fb1-92f6-b18942b3a47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"publisher\")[\"relevance_text\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee44c1c6-39ff-422a-baf1-dbee62383efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Text lenght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a028855c-a296-4819-af50-175cf8dc050c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"text_length\"] = df[\"text\"].str.split().str.len()\n",
    "max_length = int(df[\"text_length\"].max())\n",
    "bins = range(0, max_length + 500, 500)    \n",
    "\n",
    "df[\"text_length\"].hist(bins=bins)\n",
    "plt.xlabel(\"Text length (words)\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "plt.title(\"Distribution of Article Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9721bbd-6774-4bc7-8d8c-6e61bc3b516f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "time trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c71ba0-fdef-405a-ac91-e48b8b570227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df.groupby(df[\"date\"].dt.date)[\"relevance_text\"].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f9e7ed-5120-419f-b0c6-c79f24c5d31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "daily_relevance = df.groupby(df[\"date\"].dt.date)[\"relevance_text\"].mean()\n",
    "spike_days = daily_relevance.sort_values(ascending=False).head(10)\n",
    "print(spike_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4f6a73-6603-4964-8a86-6978e0095fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, filter to relevant articles only\n",
    "relevant_df = df[df[\"relevance_text\"] == 1]\n",
    "\n",
    "# Then get value counts with ratios\n",
    "relevant_ratio_by_flag = relevant_df[[\"is_us_election\", \"is_french_election\"]].sum() / len(df)\n",
    "print(relevant_ratio_by_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d16e323-1200-430c-8c67-85afabfcb11a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature engineering\n",
    "\t•\tI used spaCy, an NLP library in Python, for text analysis.\n",
    "\t•\tI used its en_core_web_sm pre-trained English language model.\n",
    "\t•\tI wrote a function to generate reproducible features from the text data.\n",
    "\t•\tThe features included:\n",
    "\t•\tNews organization name, extracted from the article URL.\n",
    "\t•\tDate-related features:\n",
    "\t•\tMonth\n",
    "\t•\tDay of week\n",
    "\t•\tWeekend indicator\n",
    "\t•\tElection countdown flags:\n",
    "\t•\tNumber of days until the US and French elections.\n",
    "\t•\tWord count of each article.\n",
    "\t•\tVerb count, using spaCy’s part-of-speech tagging.\n",
    "\t•\tElection keyword flag: if the article text mentions keywords like “election”, “vote”, “poll”, etc.\n",
    "\t•\tArticle type classification (e.g., analysis, interview, or other) based on keyword presence.\n",
    "\t•\tSome features were left out for now because they depend on extracted entities, which I’m still generating. These include:\n",
    "\t•\tEntity count\n",
    "\t•\tEntity-to-word ratio\n",
    "\t•\tWhether the article mentions any organizations\n",
    "\t•\tAfter generating these features, I ran the function on my full DataFrame.\n",
    "\t•\tI then saved the resulting featurized DataFrame to CSV for future modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e164a855-707a-4c73-9005-f370647c8ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a98810-bd11-4e50-8ae6-550b30fb6872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Load spaCy model (ensure it's installed with: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_article_features(\n",
    "    df,\n",
    "    text_col=\"text\",\n",
    "    url_col=\"url\",\n",
    "    date_col=\"datetime\",\n",
    "    #entities_col=\"extracted_entities\"\n",
    "):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Make sure date is datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "    # --- News org name (from URL)\n",
    "    df[\"news_org\"] = df[url_col].apply(lambda x: urlparse(x).netloc if pd.notnull(x) else None)\n",
    "\n",
    "    # --- Date features\n",
    "    df[\"date\"] = df[date_col].dt.date\n",
    "    df[\"day_of_week\"] = df[date_col].dt.day_name()\n",
    "    df[\"month\"] = df[date_col].dt.month\n",
    "    df[\"is_weekend\"] = df[date_col].dt.weekday >= 5\n",
    "    #df[\"time_published\"] = df[date_col].dt.time\n",
    "\n",
    "    # --- Election countdown flags (you can customize dates)\n",
    "    us_election_date = pd.to_datetime(\"2024-11-05\")\n",
    "    french_election_date = pd.to_datetime(\"2024-07-07\")\n",
    "    df[\"days_until_us_election\"] = (us_election_date - df[date_col]).dt.days\n",
    "    df[\"days_until_french_election\"] = (french_election_date - df[date_col]).dt.days\n",
    "\n",
    "    # --- Word count\n",
    "    df[\"word_count\"] = df[text_col].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "\n",
    "    # --- Verb count\n",
    "    def count_verbs(text):\n",
    "        if not isinstance(text, str): return 0\n",
    "        doc = nlp(text)\n",
    "        return sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    df[\"verb_count\"] = df[text_col].apply(count_verbs)\n",
    "\n",
    "    # # --- Entity count & ratio of entities to word count\n",
    "    # def count_entities(entities):\n",
    "    #     if not isinstance(entities, dict): return 0\n",
    "    #     return sum(len(v) for v in entities.values())\n",
    "    \n",
    "    # df[\"entity_count\"] = df[entities_col].apply(count_entities)\n",
    "    # df[\"entity_to_word_ratio\"] = df[\"entity_count\"] / df[\"word_count\"].replace(0, np.nan)\n",
    "\n",
    "    # --- Election keyword flag\n",
    "    election_keywords = [\"election\", \"vote\", \"poll\", \"ballot\", \"campaign\", \"runoff\"]\n",
    "    df[\"mentions_election_keywords\"] = df[text_col].str.contains(\"|\".join(election_keywords), case=False, na=False)\n",
    "\n",
    "    # # --- Mentions organization flag\n",
    "    # df[\"mentions_organizations\"] = df[entities_col].apply(\n",
    "    #     lambda x: isinstance(x, dict) and len(x.get(\"institutions\", [])) > 0\n",
    "    # )\n",
    "\n",
    "    # --- Language (pass-through if already exists, else dummy 'en')\n",
    "    if \"language\" not in df.columns:\n",
    "        df[\"language\"] = \"en\"\n",
    "\n",
    "    # --- Article type (simple heuristic)\n",
    "    def get_article_type(text):\n",
    "        if not isinstance(text, str): return \"other\"\n",
    "        text = text.lower()\n",
    "        if \"analysis\" in text: return \"analysis\"\n",
    "        if \"interview\" in text: return \"interview\"\n",
    "        return \"other\"\n",
    "    df[\"article_type\"] = df[text_col].apply(get_article_type)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928e43ff-26fd-4330-96a6-f926e3fd6bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_featurized = generate_article_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a08338-1c2a-40ae-9590-07a03fac3b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53f40f7-dce3-4f17-9a9b-74532d4e7fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_featurized.to_csv(\"df_featurized_us_french_no_entities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d2f1a0-074b-4a2e-98ad-c0fd23b15f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA, cleaning, FE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
